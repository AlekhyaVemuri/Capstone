{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8461986d-3ee8-471d-abb8-efc8cddc943f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, LabelEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Required for BERT\n",
    "\n",
    "# Check for GPU\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available (Training will be slow)\")\n",
    "\n",
    "# --- Helper Function to Read Data ---\n",
    "def get_lines(filename):\n",
    "    \"\"\"Reads filename and returns the lines of text as a list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries of abstract line data.\n",
    "    Takes in filename, reads it and sorts through each line,\n",
    "    extracting things like the target label, the text, etc.\n",
    "    \"\"\"\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "    \n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"): # Check if it's an ID line\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset abstract string\n",
    "        elif line.isspace(): # Check if it's a new line (end of abstract)\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "            \n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {}\n",
    "                target_text_split = abstract_line.split(\"\\t\") # Split label and text\n",
    "                line_data[\"target\"] = target_text_split[0] # Label (e.g., BACKGROUND)\n",
    "                line_data[\"text\"] = target_text_split[1].lower() # Text content\n",
    "                line_data[\"line_number\"] = abstract_line_number # Position in abstract\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split) - 1 \n",
    "                abstract_samples.append(line_data)\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "            \n",
    "    return abstract_samples\n",
    "\n",
    "# --- Evaluation Metric Function ---\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"Calculates model accuracy, precision, recall and f1 score.\"\"\"\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\": model_precision,\n",
    "                     \"recall\": model_recall,\n",
    "                     \"f1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bb2cb-fed8-40e4-b4b0-c1ec42958767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THIS PATH to where your dataset is located locally\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "train_path = data_dir + \"train.txt\"\n",
    "val_path = data_dir + \"dev.txt\"\n",
    "test_path = data_dir + \"test.txt\"\n",
    "\n",
    "# Process data\n",
    "train_samples = preprocess_text_with_line_numbers(train_path)\n",
    "val_samples = preprocess_text_with_line_numbers(val_path)\n",
    "test_samples = preprocess_text_with_line_numbers(test_path)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "# Extract sentences and labels\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "\n",
    "# One-hot encode labels (for Deep Learning)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Label Encode labels (for Baseline)\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# Get class names and vocabulary size\n",
    "class_names = label_encoder.classes_\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e205328-cb43-49f5-858f-128331e1372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, y=train_labels_encoded)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded, y_pred=baseline_preds)\n",
    "print(f\"Baseline Results: {baseline_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a57ee2-123e-4161-9c90-fdfd5de2ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Text Vectorization\n",
    "max_tokens = 68000 # Approx number of words in vocab\n",
    "avg_sent_len = int(np.mean([len(i.split()) for i in train_sentences])) # Average sentence length\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=55) # padded to 55\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "# 2. Create Embedding Layer\n",
    "token_embed = layers.Embedding(input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "                               output_dim=128,\n",
    "                               mask_zero=True,\n",
    "                               name=\"token_embedding\")\n",
    "\n",
    "# 3. Build Model\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "token_embeddings = token_embed(text_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_conv1d_token\")\n",
    "\n",
    "# 4. Compile and Fit\n",
    "model_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history_1 = model_1.fit(tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        steps_per_epoch=int(0.1 * len(train_sentences) // 32), # 10% of data for speed in demo\n",
    "                        epochs=3,\n",
    "                        validation_data=tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        validation_steps=int(0.1 * len(val_sentences) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe4337-8d56-4066-bae8-31a8089433e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Universal Sentence Encoder\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")\n",
    "\n",
    "# Build Model\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_USE\")\n",
    "\n",
    "# Compile and Fit\n",
    "model_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history_2 = model_2.fit(tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        steps_per_epoch=int(0.1 * len(train_sentences) // 32),\n",
    "                        epochs=3,\n",
    "                        validation_data=tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        validation_steps=int(0.1 * len(val_sentences) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f624e-b64d-4331-8d6f-8ec35f4fe66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Character Vectorizer\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "\n",
    "char_vectorizer = layers.TextVectorization(max_tokens=28, output_sequence_length=290, standardize=\"lower_and_strip_punctuation\")\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "char_embed = layers.Embedding(input_dim=len(char_vectorizer.get_vocabulary()), output_dim=25, mask_zero=True, name=\"char_embed\")\n",
    "\n",
    "# Build Model\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_conv1d_char\")\n",
    "\n",
    "# Compile and Fit\n",
    "model_3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history_3 = model_3.fit(tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        steps_per_epoch=int(0.1 * len(train_sentences) // 32),\n",
    "                        epochs=3,\n",
    "                        validation_data=tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        validation_steps=int(0.1 * len(val_sentences) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aaa89-610b-43fa-9a6c-10764014ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a smaller BERT model for speed/memory efficiency (Small BERT)\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "\n",
    "# Build Model\n",
    "text_input = layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "encoder_inputs = bert_preprocess_model(text_input)\n",
    "outputs = bert_model(encoder_inputs)\n",
    "net = outputs['pooled_output'] # Pooling the sequence of outputs\n",
    "net = layers.Dropout(0.1)(net)\n",
    "net = layers.Dense(64, activation='relu')(net)\n",
    "final_output = layers.Dense(len(class_names), activation='softmax')(net)\n",
    "\n",
    "model_4 = tf.keras.Model(text_input, final_output)\n",
    "\n",
    "# Compile and Fit\n",
    "model_4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history_4 = model_4.fit(tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        steps_per_epoch=int(0.1 * len(train_sentences) // 32),\n",
    "                        epochs=3,\n",
    "                        validation_data=tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE),\n",
    "                        validation_steps=int(0.1 * len(val_sentences) // 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47702f5-24dc-46f8-8410-1558e6571ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results\n",
    "model_1_preds = model_1.predict(val_sentences)\n",
    "model_1_results = calculate_results(y_true=val_labels_encoded, y_pred=tf.argmax(model_1_preds, axis=1))\n",
    "\n",
    "model_2_preds = model_2.predict(val_sentences)\n",
    "model_2_results = calculate_results(y_true=val_labels_encoded, y_pred=tf.argmax(model_2_preds, axis=1))\n",
    "\n",
    "model_3_preds = model_3.predict(val_chars)\n",
    "model_3_results = calculate_results(y_true=val_labels_encoded, y_pred=tf.argmax(model_3_preds, axis=1))\n",
    "\n",
    "model_4_preds = model_4.predict(val_sentences)\n",
    "model_4_results = calculate_results(y_true=val_labels_encoded, y_pred=tf.argmax(model_4_preds, axis=1))\n",
    "\n",
    "# Combine into DataFrame\n",
    "all_results = pd.DataFrame({\n",
    "    \"Baseline\": baseline_results,\n",
    "    \"Conv1D Token\": model_1_results,\n",
    "    \"Pretrained (USE)\": model_2_results,\n",
    "    \"Conv1D Char\": model_3_results,\n",
    "    \"BERT (LLM)\": model_4_results\n",
    "}).transpose()\n",
    "\n",
    "# Scale accuracy to 0-1\n",
    "all_results[\"accuracy\"] = all_results[\"accuracy\"] / 100\n",
    "\n",
    "# Plot\n",
    "all_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.show()\n",
    "\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e4479-dd55-4a04-88a0-8a73fd591b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
